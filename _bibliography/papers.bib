---
---

@string{aps = {American Physical Society,}}

@article{grover2022neurips,
  arxiv = {2209.13017},
  honor = {Oral (Spotlight) paper},
  abbr = {NeurIPS},
  title={Public Wisdom Matters! Discourse-Aware Hyperbolic Fourier Co-Attention for Social-Text Classification},
  author="Grover, Karish and
      Angara, Phaneendra  and
      Akhtar, Md. Shad  and
      Chakraborty, Tanmoy",
  journal={Proceedings of the Thirty-Sixth Annual Conference on Neural Information Processing Systems},
  month = sep,
  year={2022},
  code = {https://github.com/LCS2-IIITD/Hyphen},
  publisher={Courier Corporation,},
  preview={neurips_preview.png},
  selected = {true},
  abstract = {Social media has become the fulcrum of all forms of communication. Classifying social texts such as fake news, rumour, sarcasm, etc. has gained significant attention. The surface-level signals expressed by a social-text itself may not be adequate for such tasks; therefore, recent methods attempted to incorporate other intrinsic signals such as user behavior and the underlying graph structure. Oftentimes, the "public wisdom" expressed through the comments/replies to a social-text acts as a surrogate of crowd-sourced view and may provide us with complementary signals. State-of-the-art methods on social-text classification tend to ignore such a rich hierarchical signal. Here, we propose Hyphen, a discourse-aware hyperbolic spec- tral co-attention network. Hyphen is a fusion of hyperbolic graph representation learning with a novel Fourier co-attention mechanism in an attempt to generalise the social-text classification tasks by incorporating public discourse. We parse public discourse as an Abstract Meaning Representation (AMR) graph and use the powerful hyperbolic geometric representation to model graphs with hierarchical structure. Finally, we equip it with a novel Fourier co-attention mechanism to capture the correlation between the source post and public discourse. Extensive experiments on four different social-text classification tasks, namely detecting fake news, hate speech, rumour, and sarcasm, show that Hyphen generalises well, and achieves state-of-the-art results on ten benchmark datasets. We also employ a sentence-level fact-checked and annotated dataset to evaluate how Hyphen is capable of producing explanations as analogous evidence to the final prediction.}}

@article{agarwal-etal-2022-multi,
    abbr = {NAACL},
    title = "Multi-Relational Graph Transformer for Automatic Short Answer Grading",
    author = "Agarwal, Rajat  and
      Khurana, Varun  and
      Grover, Karish  and
      Mohania, Mukesh  and
      Goyal, Vikram",
    journal = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.146",
    doi = "10.18653/v1/2022.naacl-main.146",
    pages = "2001--2012",
    abstract = "The recent transition to the online educational domain has increased the need for Automatic Short Answer Grading (ASAG). ASAG automatically evaluates a student{'}s response against a (given) correct response and thus has been a prevalent semantic matching task. Most existing methods utilize sequential context to compare two sentences and ignore the structural context of the sentence; therefore, these methods may not result in the desired performance. In this paper, we overcome this problem by proposing a Multi-Relational Graph Transformer, MitiGaTe, to prepare token representations considering the structural context. Abstract Meaning Representation (AMR) graph is created by parsing the text response and then segregated into multiple subgraphs, each corresponding to a particular relationship in AMR. A Graph Transformer is used to prepare relation-specific token embeddings within each subgraph, then aggregated to obtain a subgraph representation. Finally, we compare the correct answer and the student response subgraph representations to yield a final score. Experimental results on Mohler{'}s dataset show that our system outperforms the existing state-of-the-art methods. We have released our implementation https://github.com/kvarun07/asag-gt, as we believe that our model can be useful for many future applications.",
    selected = {true},
    preview={naacl_preview.png},
    code = {https://github.com/kvarun07/asag-gt},
    pdf = {https://aclanthology.org/2022.naacl-main.146/},
    talk = {https://aclanthology.org/2022.naacl-main.146.mp4},
    honor = {Oral paper}
}

@article{grover2021haha,
  abbr = {SEPLN},
  title={HAHA@ IberLEF2021: Humor Analysis using Ensembles of Simple Transformers.},
  author={Grover, Karish and Goel, Tanishq},
  journal={Iberian Languages Evaluation Forum},
  pages={883--890},
  year={2021},
  selected={true},
  abstract = {This paper describes the system submitted to the Humor
Analysis based on Human Annotation (HAHA) task at IberLEF 2021.
This system achieves the winning F1 score of 0.8850 in the main task
of binary classification (Task 1) utilizing an ensemble of a pre-trained
multilingual BERT, pre-trained Spanish BERT (BETO), RoBERTa, and
a naive Bayes classifier. We also achieve second place with macro F1
Scores of 0.2916 and 0.3578 in Multi-class Classification and Multi-label
Classification tasks, respectively, and third place with an RMSE score of
0.6295 in the Regression task.}, 
  code = {https://github.com/karish-grover/Humor-Analysis-using-Ensembles-of-Simple-Transformers},
  pdf = {http://ceur-ws.org/Vol-2943/haha_paper10.pdf},
  preview={sepln_preview.png},
}
