---
---

@string{aps = {American Physical Society,}}

@article{grover2022neurips,
  arxiv = {2209.13017},
  honor = {Oral (Spotlight) paper},
  abbr = {NeurIPS},
  title={Public Wisdom Matters! Discourse-Aware Hyperbolic Fourier Co-Attention for Social-Text Classification},
  author="Grover, Karish and
      Angara, Phaneendra  and
      Akhtar, Md. Shad  and
      Chakraborty, Tanmoy",
  journal={Proceedings of the Thirty-Sixth Annual Conference on Neural Information Processing Systems},
  month = sep,
  year={2022},
  code = {https://github.com/LCS2-IIITD/Hyphen},
  publisher={Courier Corporation,},
  preview={neurips_preview.png},
  selected = {true},
  abstract = {Social media has become the fulcrum of all forms of communication. Classifying social texts such as fake news, rumour, sarcasm, etc. has gained significant attention. The surface-level signals expressed by a social-text itself may not be adequate for such tasks; therefore, recent methods attempted to incorporate other intrinsic signals such as user behavior and the underlying graph structure. Oftentimes, the "public wisdom" expressed through the comments/replies to a social-text acts as a surrogate of crowd-sourced view and may provide us with complementary signals. State-of-the-art methods on social-text classification tend to ignore such a rich hierarchical signal. Here, we propose Hyphen, a discourse-aware hyperbolic spec- tral co-attention network. Hyphen is a fusion of hyperbolic graph representation learning with a novel Fourier co-attention mechanism in an attempt to generalise the social-text classification tasks by incorporating public discourse. We parse public discourse as an Abstract Meaning Representation (AMR) graph and use the powerful hyperbolic geometric representation to model graphs with hierarchical structure. Finally, we equip it with a novel Fourier co-attention mechanism to capture the correlation between the source post and public discourse. Extensive experiments on four different social-text classification tasks, namely detecting fake news, hate speech, rumour, and sarcasm, show that Hyphen generalises well, and achieves state-of-the-art results on ten benchmark datasets. We also employ a sentence-level fact-checked and annotated dataset to evaluate how Hyphen is capable of producing explanations as analogous evidence to the final prediction.}}

@article{agarwal-etal-2022-multi,
    abbr = {NAACL},
    title = "Multi-Relational Graph Transformer for Automatic Short Answer Grading",
    author = "Agarwal, Rajat  and
      Khurana, Varun  and
      Grover, Karish  and
      Mohania, Mukesh  and
      Goyal, Vikram",
    journal = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.146",
    doi = "10.18653/v1/2022.naacl-main.146",
    pages = "2001--2012",
    abstract = "The recent transition to the online educational domain has increased the need for Automatic Short Answer Grading (ASAG). ASAG automatically evaluates a student{'}s response against a (given) correct response and thus has been a prevalent semantic matching task. Most existing methods utilize sequential context to compare two sentences and ignore the structural context of the sentence; therefore, these methods may not result in the desired performance. In this paper, we overcome this problem by proposing a Multi-Relational Graph Transformer, MitiGaTe, to prepare token representations considering the structural context. Abstract Meaning Representation (AMR) graph is created by parsing the text response and then segregated into multiple subgraphs, each corresponding to a particular relationship in AMR. A Graph Transformer is used to prepare relation-specific token embeddings within each subgraph, then aggregated to obtain a subgraph representation. Finally, we compare the correct answer and the student response subgraph representations to yield a final score. Experimental results on Mohler{'}s dataset show that our system outperforms the existing state-of-the-art methods. We have released our implementation https://github.com/kvarun07/asag-gt, as we believe that our model can be useful for many future applications.",
    selected = {true},
    preview={naacl_preview.png},
    code = {https://github.com/kvarun07/asag-gt},
    pdf = {https://aclanthology.org/2022.naacl-main.146/},
    talk = {https://aclanthology.org/2022.naacl-main.146.mp4},
    honor = {Oral paper}
}

@article{grover2021haha,
  abbr = {SEPLN},
  title={HAHA@ IberLEF2021: Humor Analysis using Ensembles of Simple Transformers.},
  author={Grover, Karish and Goel, Tanishq},
  journal={Iberian Languages Evaluation Forum},
  pages={883--890},
  year={2021},
  selected={true},
  abstract = {This paper describes the system submitted to the Humor
Analysis based on Human Annotation (HAHA) task at IberLEF 2021.
This system achieves the winning F1 score of 0.8850 in the main task
of binary classification (Task 1) utilizing an ensemble of a pre-trained
multilingual BERT, pre-trained Spanish BERT (BETO), RoBERTa, and
a naive Bayes classifier. We also achieve second place with macro F1
Scores of 0.2916 and 0.3578 in Multi-class Classification and Multi-label
Classification tasks, respectively, and third place with an RMSE score of
0.6295 in the Regression task.}, 
  code = {https://github.com/karish-grover/Humor-Analysis-using-Ensembles-of-Simple-Transformers},
  pdf = {http://ceur-ws.org/Vol-2943/haha_paper10.pdf},
  preview={sepln_preview.png},
}


@article{ghosh2024dependency,
  abbr ={WWW},
  title={Dependency Aware Incident Linking in Large Cloud Systems},
  author={Ghosh, Supriyo and Grover, Karish and Wong, Jimmy and Bansal, Chetan and Namineni, Rakesh and Verma, Mohit and Rajmohan, Saravan},
  journal={Companion Proceedings of the ACM on Web Conference 2024},
  pages={141--150},
  year={2024},
  selected={true},
  preview = {dlink.png},
  pdf={https://dl.acm.org/doi/abs/10.1145/3589335.3648311},
  abstract = {Despite significant reliability efforts, large-scale cloud services inevitably experience production incidents that can significantly impact service availability and customer satisfaction. Worse, in many cases one incident can lead to multiple downstream failures due to cascading effects that create several related incidents across different dependent services. Often time On-call Engineers (OCEs) examine these incidents in silos that lead to significant amounts of manual effort and increase the overall time-to-mitigate incidents. Therefore, developing efficient incident linking models is of paramount importance for grouping related incidents into clusters so as to quickly resolve major outages and reduce on-call fatigue. Existing incident linking methods mostly leverage textual and contextual information of incidents (e.g., title, description, severity, impacted components), thus failing to leverage the inter-dependencies between services. In this paper, we propose the dependency-aware incident linking (DiLink) framework which leverages both textual and service dependency graph information to improve the accuracy and coverage of incident links not only emerge from same service, but also from different services and workloads. Furthermore, we propose a novel method to align the embeddings of multi-modal (i.e., textual and graphical) data using Orthogonal Procrustes. Extensive experimental results on real-world incidents from 5 workloads of Microsoft demonstrate that our alignment method has an F1-score of 0.96 (14\% gain over current state-of-the-art methods). We are also in the process of deploying this solution across 610 services from these 5 workloads for continuously supporting OCEs improving incident management and reducing manual effort.}
}